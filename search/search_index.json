{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"JatanaAI's documentation! JatanaAI is a deep learning based end-to-end natural language platform. JatanaAI aims to provide easy and fast experiences for using, deploying, and developing natural language processing for various industrial use cases. JatanaAI is mainly implemented using TensorFlow and Python. Prerequisites Python 2.7/3.6 Java8+ Ubuntu 16.x Key competencies Jatana NLU Engine is a highly scalable, data-driven engine with some unique capabilities like: Cross-lingual and Multilingual Understanding : It can handle emails composed of multiple languages. Domain and Language Agnostic : It can learn the domain specific concepts from the data itself and in any provided language. Advance real-time analytics : It provides advance insight on the client data like Topic Modelling, Deep Summarizations, Hot Keywords, and Sentiment Analysis. Also, it serves the real-time analysis of how the model is performing. Continuous Learning : It is capable of learning the new concepts and ideas over the period of time. Reinforcement learning, Meta Learning and Imitation learning are the key techniques used for the optimization. Note This is the documentation for version |release| of JatanaAI which is specifically designed to work on Google Cloud in microservice mode. Components of JatanaAI An overview of Jatana's internal modules looks: Data Pipelines : This module contains all the data processing and cleaning pipelines. We deployed various advance regex techniques for removing unwanted data. Also, these pipelines adjust based on the language detected. AI Core : Brain behind JatanaAI is a hybrid combination of Deep Learning architectures like CNN's and RNN's which helps to capture the context in the provided text. It is further fine-tuned by performing transfer learning using the huge multilingual language models (like ELMO and BERT) which add the capability to understand text with multiple languages. Jatana Deep Learning architectures can handle various NLP problems like compositionality, polysemy, anaphora, long-term dependencies and bi-directional context. Optimization : This module is used to boost the performance of the existing model using the mechanism called continuous learning. This module also performs post training optimization on the models by various methods like Quantization, Pruning and Compress. Serving : This modules make JatanaAI planet scalable solution with the power of Tensorflow Serving. It uses GRPC and HTTP protocals to make the million request per second and auto manage the model's versioning. Sample Usage Assume that you draft an email to the customer support, JatanaAI interprets the email, extracts the semantic and syntactic relations and responds with the list of most relevant answer. Example Hi Team, Last week I have ordered a product from your web shop after seeing an ad on my Facebook feed. While completing the purchase I\u2019ve selected the express shipment option to make sure I would receive the product as fast as possible. More than a week has passed but and I haven\u2019t received anything yet! Can you please check what\u2019s going on with my shipment? Thanks, Frenk and returns structured response like { possible_answers : [ { answer1 : SHIPMENT_ISSUE , confidence : 0.93, answer_id : 360008597720 }, { answer2 : WRONG_PRODUCT , confidence : 0.08, answer_id : 360008661139 }, { answer3 : OPENING_HOURS , confidence : 0.07, answer_id : 360008598900 }, { answer4 : PRODUCT_LAUNCHES , confidence : 0.06, answer_id : 360008661339 }, { answer5 : REPAIRS , confidence : 0.05, answer_id : 360008661359 } ], client : jatana , }","title":"Get Started"},{"location":"#jatanaais-documentation","text":"JatanaAI is a deep learning based end-to-end natural language platform. JatanaAI aims to provide easy and fast experiences for using, deploying, and developing natural language processing for various industrial use cases. JatanaAI is mainly implemented using TensorFlow and Python.","title":"JatanaAI's documentation!"},{"location":"#prerequisites","text":"Python 2.7/3.6 Java8+ Ubuntu 16.x","title":"Prerequisites"},{"location":"#key-competencies","text":"Jatana NLU Engine is a highly scalable, data-driven engine with some unique capabilities like: Cross-lingual and Multilingual Understanding : It can handle emails composed of multiple languages. Domain and Language Agnostic : It can learn the domain specific concepts from the data itself and in any provided language. Advance real-time analytics : It provides advance insight on the client data like Topic Modelling, Deep Summarizations, Hot Keywords, and Sentiment Analysis. Also, it serves the real-time analysis of how the model is performing. Continuous Learning : It is capable of learning the new concepts and ideas over the period of time. Reinforcement learning, Meta Learning and Imitation learning are the key techniques used for the optimization. Note This is the documentation for version |release| of JatanaAI which is specifically designed to work on Google Cloud in microservice mode.","title":"Key competencies"},{"location":"#components-of-jatanaai","text":"An overview of Jatana's internal modules looks: Data Pipelines : This module contains all the data processing and cleaning pipelines. We deployed various advance regex techniques for removing unwanted data. Also, these pipelines adjust based on the language detected. AI Core : Brain behind JatanaAI is a hybrid combination of Deep Learning architectures like CNN's and RNN's which helps to capture the context in the provided text. It is further fine-tuned by performing transfer learning using the huge multilingual language models (like ELMO and BERT) which add the capability to understand text with multiple languages. Jatana Deep Learning architectures can handle various NLP problems like compositionality, polysemy, anaphora, long-term dependencies and bi-directional context. Optimization : This module is used to boost the performance of the existing model using the mechanism called continuous learning. This module also performs post training optimization on the models by various methods like Quantization, Pruning and Compress. Serving : This modules make JatanaAI planet scalable solution with the power of Tensorflow Serving. It uses GRPC and HTTP protocals to make the million request per second and auto manage the model's versioning.","title":"Components of JatanaAI"},{"location":"#sample-usage","text":"Assume that you draft an email to the customer support, JatanaAI interprets the email, extracts the semantic and syntactic relations and responds with the list of most relevant answer. Example Hi Team, Last week I have ordered a product from your web shop after seeing an ad on my Facebook feed. While completing the purchase I\u2019ve selected the express shipment option to make sure I would receive the product as fast as possible. More than a week has passed but and I haven\u2019t received anything yet! Can you please check what\u2019s going on with my shipment? Thanks, Frenk and returns structured response like { possible_answers : [ { answer1 : SHIPMENT_ISSUE , confidence : 0.93, answer_id : 360008597720 }, { answer2 : WRONG_PRODUCT , confidence : 0.08, answer_id : 360008661139 }, { answer3 : OPENING_HOURS , confidence : 0.07, answer_id : 360008598900 }, { answer4 : PRODUCT_LAUNCHES , confidence : 0.06, answer_id : 360008661339 }, { answer5 : REPAIRS , confidence : 0.05, answer_id : 360008661359 } ], client : jatana , }","title":"Sample Usage"},{"location":"apis/","text":"Jatana API endpoints For Upload API's Ignition Route To test if the service is up and running. Request GET http://localhost:8080/api/ms/v3.0/ignition Response { Engine Status : Microservice for UPLOAD is alive and kicking } Upload Route This API is used to send the JSON data to the Upload service .. todo:: Add data format reference or sample Request import requests url = http://localhost:8080/api/ms/v3.0/upload querystring = { client : netto } payload = { [{\\ ticket\\ : {\\ via\\ : {\\ source\\ : {\\ to\\ : {\\ name\\ : \\ XYZ\\ , \\HTML, like Gecko) Chrome/47.0.2526.80 Safari/537.36\\ , \\ ip_address\\ : \\ 80.254.154.59\\ , \\ location\\ : \\ Denmark\\ , \\ longitude\\ : 12.056399999999996}, \\ custom\\ : {}}}]}]} headers = { 'Content-Type': application/json , } response = requests.request( POST , url, data=payload, headers=headers, params=querystring) print(response.text) Response { 'Message' : 'Training Successsful' } Training Route .. warning:: This endpoint is for internal use only. This training endpoint is a backdoor to train any model on ML ENGINE which is only to be used by the AI Team. .. todo:: Add params for lang Request import requests url = https://localhost:8080/api/ms/v3.0/train querystring = { client : new_client } payload = response = requests.request( GET , url, data=payload, params=querystring) print(response.text) Response { 'job_name' : 'new_cleint_ml_jos12344', 'job_result': 'Model Trained' } For Query API's Query Route [TF Serving Version] .. note:: The instances object in json needs to be a list of int of 100 elements. Request curl -X POST \\ http://localhost:8501/v1/models/jatanademo_macro:predict \\ -d '{ signature_name : predict , instances : [ [ 33, 114, 9, 30, 0, 30, 0, 50, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]}' Response { predictions : [[ 2.36913e-16, 5.55834e-32, 2.54004e-7, 1, 4.11396e-10 ] ]} Query Route [WRAPPER VERSION] Request .. todo:: Fix Payload import requests url = http://localhost:8080/api/ms/v3.0/query payload = Content-Disposition: form-data; name=\\ q\\ \\r\\n\\r\\n\\ Hi,\\n\\nLast week I have ordered a product from your web shop after seeing an ad on my Facebook feed.\\n\\nWhile completing the purchase I\u2019ve selected the express shipment option to make sure I would receive the product as fast as possible.\\n\\nMore than a week has passed but and I haven\u2019t received anything yet!\\n\\nCan you please check what\u2019s going on with my shipment?\\n\\nThanks,\\n\\nFrenk\\ \\r\\n------WebKitFormBoundary7MA4YWxkTrZu0gW\\r\\nContent-Disposition: form-data; name=\\ client\\ \\r\\n\\r\\njatana\\r\\n headers = { 'content-type': multipart/form-data , 'Authorization': Bearer TOKEN } response = requests.request( POST , url, data=payload, headers=headers) print(response.text) Response { macros : [ { macro_title : SHIPMENT_ISSUE , confidence : 0.93, macro_id : 360008597720 }, { macro_title : WRONG_PRODUCT , confidence : 0.08, macro_id : 360008661139 }, { macro_title : OPENING_HOURS , confidence : 0.07, macro_id : 360008598900 }, { macro_title : PRODUCT_LAUNCHES , confidence : 0.06, macro_id : 360008661339 }, { macro_title : REPAIRS , confidence : 0.05, macro_id : 360008661359 } ], suggested_routes : [], client : jatana , suggested_tags : [] }","title":"Endpoints"},{"location":"apis/#jatana-api-endpoints","text":"","title":"Jatana API endpoints"},{"location":"apis/#for-upload-apis","text":"","title":"For Upload API's"},{"location":"apis/#ignition-route","text":"To test if the service is up and running. Request GET http://localhost:8080/api/ms/v3.0/ignition Response { Engine Status : Microservice for UPLOAD is alive and kicking }","title":"Ignition Route"},{"location":"apis/#upload-route","text":"This API is used to send the JSON data to the Upload service .. todo:: Add data format reference or sample Request import requests url = http://localhost:8080/api/ms/v3.0/upload querystring = { client : netto } payload = { [{\\ ticket\\ : {\\ via\\ : {\\ source\\ : {\\ to\\ : {\\ name\\ : \\ XYZ\\ , \\HTML, like Gecko) Chrome/47.0.2526.80 Safari/537.36\\ , \\ ip_address\\ : \\ 80.254.154.59\\ , \\ location\\ : \\ Denmark\\ , \\ longitude\\ : 12.056399999999996}, \\ custom\\ : {}}}]}]} headers = { 'Content-Type': application/json , } response = requests.request( POST , url, data=payload, headers=headers, params=querystring) print(response.text) Response { 'Message' : 'Training Successsful' }","title":"Upload Route"},{"location":"apis/#training-route","text":".. warning:: This endpoint is for internal use only. This training endpoint is a backdoor to train any model on ML ENGINE which is only to be used by the AI Team. .. todo:: Add params for lang Request import requests url = https://localhost:8080/api/ms/v3.0/train querystring = { client : new_client } payload = response = requests.request( GET , url, data=payload, params=querystring) print(response.text) Response { 'job_name' : 'new_cleint_ml_jos12344', 'job_result': 'Model Trained' }","title":"Training Route"},{"location":"apis/#for-query-apis","text":"","title":"For Query API's"},{"location":"apis/#query-route-tf-serving-version","text":".. note:: The instances object in json needs to be a list of int of 100 elements. Request curl -X POST \\ http://localhost:8501/v1/models/jatanademo_macro:predict \\ -d '{ signature_name : predict , instances : [ [ 33, 114, 9, 30, 0, 30, 0, 50, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]}' Response { predictions : [[ 2.36913e-16, 5.55834e-32, 2.54004e-7, 1, 4.11396e-10 ] ]}","title":"Query Route [TF Serving Version]"},{"location":"apis/#query-route-wrapper-version","text":"Request .. todo:: Fix Payload import requests url = http://localhost:8080/api/ms/v3.0/query payload = Content-Disposition: form-data; name=\\ q\\ \\r\\n\\r\\n\\ Hi,\\n\\nLast week I have ordered a product from your web shop after seeing an ad on my Facebook feed.\\n\\nWhile completing the purchase I\u2019ve selected the express shipment option to make sure I would receive the product as fast as possible.\\n\\nMore than a week has passed but and I haven\u2019t received anything yet!\\n\\nCan you please check what\u2019s going on with my shipment?\\n\\nThanks,\\n\\nFrenk\\ \\r\\n------WebKitFormBoundary7MA4YWxkTrZu0gW\\r\\nContent-Disposition: form-data; name=\\ client\\ \\r\\n\\r\\njatana\\r\\n headers = { 'content-type': multipart/form-data , 'Authorization': Bearer TOKEN } response = requests.request( POST , url, data=payload, headers=headers) print(response.text) Response { macros : [ { macro_title : SHIPMENT_ISSUE , confidence : 0.93, macro_id : 360008597720 }, { macro_title : WRONG_PRODUCT , confidence : 0.08, macro_id : 360008661139 }, { macro_title : OPENING_HOURS , confidence : 0.07, macro_id : 360008598900 }, { macro_title : PRODUCT_LAUNCHES , confidence : 0.06, macro_id : 360008661339 }, { macro_title : REPAIRS , confidence : 0.05, macro_id : 360008661359 } ], suggested_routes : [], client : jatana , suggested_tags : [] }","title":"Query Route [WRAPPER VERSION]"},{"location":"installation/","text":"Installation Prerequisites JatanaAI has been tested with Python 2.7 and 3.6. It is recommended to use it with Python 2.7, we will soon port the complete code on Python3.7 but its not going to impact any interfaces. There are primarily 2 server which we need to setup: Upload server Query Serving Server Note Setting up JatanaAI for lots of clients can require high configuration of systems for inferencing. Setting up Upload Server Follow the instruction to setup the upload server. Config server sudo apt-get update sudo apt-get install apt-transport-https ca-certificates curl gnupg-agent software-properties-common Get the code Clone the repository git clone https://github.com/jatana-ai/jatana-nlp-microservices Unless you've already got numpy scipy installed, we highly recommend that you install and use Anaconda . cd jatana-nlp-microservices # create a virtual environment virtualenv -p python3.6 JatanaAI # activate the virtual environment source jatana-nlp-microservices/bin/activate # install dependencies pip install -r requirements.txt # install gcfuse sudo apt-get install gcsfuse # setup storage using gcfuse mkdir data gcsfuse jatana-nlp-storage data/ # run service ./jatana-ms-upload/ignite.sh Test JatanaAI GET locahost:8080/api/ms/v3.0/ignition and returns response { Engine Status : Microservice for UPLOAD is alive and kicking } Setting up Serving Server Config server sudo apt-get update sudo apt-get install apt-transport-https ca-certificates curl gnupg-agent software-properties-common sudo apt-get install gcsfuse sudo apt install docker.io Setup GCFuse export GCSFUSE_REPO=gcsfuse-`lsb_release -c -s` echo deb http://packages.cloud.google.com/apt $GCSFUSE_REPO main | sudo tee /etc/apt/sources.list.d/gcsfuse.list curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add - sudo apt-get install gcsfuse Get the code Clone the repository git clone https://github.com/jatana-ai/jatana-nlp-microservices cd /jatana-nlp-microservices/jatana-ms-train-v2/trainer mkdir data gcsfuse --implicit-dirs jatana-nlp-storage data/ Get Tensorflow serving docker image docker pull tensorflow/serving Run the serving docker run -p 8500:8500 -p 8501:8501 --mount type=bind,source=/home/projects/jatana-nlp-microservices/jatana-ms-train-v2/trainer/data/export/,target=/models/ -t tensorflow/serving --model_config_file=/models/models.config Testing If everything went fine then the service will be up and running and we can test that by hitting multiple enpoints. Model status API If you want to check the status of the model if can be done using Request GET http://host:port/v1/models/${MODEL_NAME}[/versions/${MODEL_VERSION}] /versions/${MODEL_VERSION} is optional. If omitted status for all versions is returned in the response. Response { model_version_status : [ { version : 3 , state : AVAILABLE , status : { error_code : OK , error_message : } } ] } Model Metadata API It returns the metadata of a model in the ModelServer. It returns the metadata of a model in the ModelServer. Request GET http://host:port/v1/models/${MODEL_NAME}[/versions/${MODEL_VERSION}]/metadata /versions/${MODEL_VERSION} is optional. If omitted status for all versions is returned in the response. Example: GET http://localhost:8501/v1/models/jatanademo_macro/metadata Response { model_spec : { name : jatanademo_macro , signature_name : , version : 3 }, metadata : { signature_def : { signature_def : { predict : { inputs : { text : { dtype : DT_INT32 , tensor_shape : { dim : [ { size : -1 , name : }, { size : 100 , name : } ], unknown_rank : false }, name : model_1_input:0 } }, outputs : { scores : { dtype : DT_FLOAT , tensor_shape : { dim : [ { size : -1 , name : }, { size : 5 , name : } ], unknown_rank : false }, name : dense_1/Softmax:0 } }, method_name : tensorflow/serving/predict } } } } }","title":"Installation"},{"location":"installation/#installation","text":"","title":"Installation"},{"location":"installation/#prerequisites","text":"JatanaAI has been tested with Python 2.7 and 3.6. It is recommended to use it with Python 2.7, we will soon port the complete code on Python3.7 but its not going to impact any interfaces. There are primarily 2 server which we need to setup: Upload server Query Serving Server Note Setting up JatanaAI for lots of clients can require high configuration of systems for inferencing.","title":"Prerequisites"},{"location":"installation/#setting-up-upload-server","text":"Follow the instruction to setup the upload server.","title":"Setting up Upload Server"},{"location":"installation/#config-server","text":"sudo apt-get update sudo apt-get install apt-transport-https ca-certificates curl gnupg-agent software-properties-common","title":"Config server"},{"location":"installation/#get-the-code","text":"Clone the repository git clone https://github.com/jatana-ai/jatana-nlp-microservices Unless you've already got numpy scipy installed, we highly recommend that you install and use Anaconda . cd jatana-nlp-microservices # create a virtual environment virtualenv -p python3.6 JatanaAI # activate the virtual environment source jatana-nlp-microservices/bin/activate # install dependencies pip install -r requirements.txt # install gcfuse sudo apt-get install gcsfuse # setup storage using gcfuse mkdir data gcsfuse jatana-nlp-storage data/ # run service ./jatana-ms-upload/ignite.sh","title":"Get the code"},{"location":"installation/#test-jatanaai","text":"GET locahost:8080/api/ms/v3.0/ignition and returns response { Engine Status : Microservice for UPLOAD is alive and kicking }","title":"Test JatanaAI"},{"location":"installation/#setting-up-serving-server","text":"","title":"Setting up Serving Server"},{"location":"installation/#config-server_1","text":"sudo apt-get update sudo apt-get install apt-transport-https ca-certificates curl gnupg-agent software-properties-common sudo apt-get install gcsfuse sudo apt install docker.io","title":"Config server"},{"location":"installation/#setup-gcfuse","text":"export GCSFUSE_REPO=gcsfuse-`lsb_release -c -s` echo deb http://packages.cloud.google.com/apt $GCSFUSE_REPO main | sudo tee /etc/apt/sources.list.d/gcsfuse.list curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add - sudo apt-get install gcsfuse","title":"Setup GCFuse"},{"location":"installation/#get-the-code_1","text":"Clone the repository git clone https://github.com/jatana-ai/jatana-nlp-microservices cd /jatana-nlp-microservices/jatana-ms-train-v2/trainer mkdir data gcsfuse --implicit-dirs jatana-nlp-storage data/","title":"Get the code"},{"location":"installation/#get-tensorflow-serving-docker-image","text":"docker pull tensorflow/serving","title":"Get Tensorflow serving docker image"},{"location":"installation/#run-the-serving","text":"docker run -p 8500:8500 -p 8501:8501 --mount type=bind,source=/home/projects/jatana-nlp-microservices/jatana-ms-train-v2/trainer/data/export/,target=/models/ -t tensorflow/serving --model_config_file=/models/models.config","title":"Run the serving"},{"location":"installation/#testing","text":"If everything went fine then the service will be up and running and we can test that by hitting multiple enpoints.","title":"Testing"},{"location":"installation/#model-status-api","text":"If you want to check the status of the model if can be done using Request GET http://host:port/v1/models/${MODEL_NAME}[/versions/${MODEL_VERSION}] /versions/${MODEL_VERSION} is optional. If omitted status for all versions is returned in the response. Response { model_version_status : [ { version : 3 , state : AVAILABLE , status : { error_code : OK , error_message : } } ] }","title":"Model status API"},{"location":"installation/#model-metadata-api","text":"It returns the metadata of a model in the ModelServer. It returns the metadata of a model in the ModelServer. Request GET http://host:port/v1/models/${MODEL_NAME}[/versions/${MODEL_VERSION}]/metadata /versions/${MODEL_VERSION} is optional. If omitted status for all versions is returned in the response. Example: GET http://localhost:8501/v1/models/jatanademo_macro/metadata Response { model_spec : { name : jatanademo_macro , signature_name : , version : 3 }, metadata : { signature_def : { signature_def : { predict : { inputs : { text : { dtype : DT_INT32 , tensor_shape : { dim : [ { size : -1 , name : }, { size : 100 , name : } ], unknown_rank : false }, name : model_1_input:0 } }, outputs : { scores : { dtype : DT_FLOAT , tensor_shape : { dim : [ { size : -1 , name : }, { size : 5 , name : } ], unknown_rank : false }, name : dense_1/Softmax:0 } }, method_name : tensorflow/serving/predict } } } } }","title":"Model Metadata API"},{"location":"language/","text":".. _language: Supported Languages Jatana speakes 120 languages , which are Afrikaans Albanian Arabic Aragonese Armenian Asturian Azerbaijani Bashkir Basque Bavarian Belarusian Bengali Bishnupriya Manipuri Bosnian Breton Bulgarian Burmese Catalan Cebuano Chechen Chinese (Simplified) Chinese (Traditional) Chuvash Croatian Czech Danish Dutch English Estonian Finnish French Galician Georgian German Greek Gujarati Haitian Hebrew Hindi Hungarian Icelandic Ido Indonesian Irish Italian Japanese Javanese Kannada Kazakh Kirghiz Korean Latin Latvian Lithuanian Lombard Low Saxon Luxembourgish Macedonian Malagasy Malay Malayalam Marathi Minangkabau Nepali Newar Norwegian (Bokmal) Norwegian (Nynorsk) Occitan Persian (Farsi) Piedmontese Polish Portuguese Punjabi Romanian Russian Scots Serbian Serbo-Croatian Sicilian Slovak Slovenian South Azerbaijani Spanish Sundanese Swahili Swedish Tagalog Tajik Tamil Tatar Telugu Turkish Ukrainian Urdu Uzbek Vietnamese Volap\u00fck Waray-Waray Welsh West Frisian Western Punjabi Yoruba","title":"Language"},{"location":"language/#supported-languages","text":"Jatana speakes 120 languages , which are Afrikaans Albanian Arabic Aragonese Armenian Asturian Azerbaijani Bashkir Basque Bavarian Belarusian Bengali Bishnupriya Manipuri Bosnian Breton Bulgarian Burmese Catalan Cebuano Chechen Chinese (Simplified) Chinese (Traditional) Chuvash Croatian Czech Danish Dutch English Estonian Finnish French Galician Georgian German Greek Gujarati Haitian Hebrew Hindi Hungarian Icelandic Ido Indonesian Irish Italian Japanese Javanese Kannada Kazakh Kirghiz Korean Latin Latvian Lithuanian Lombard Low Saxon Luxembourgish Macedonian Malagasy Malay Malayalam Marathi Minangkabau Nepali Newar Norwegian (Bokmal) Norwegian (Nynorsk) Occitan Persian (Farsi) Piedmontese Polish Portuguese Punjabi Romanian Russian Scots Serbian Serbo-Croatian Sicilian Slovak Slovenian South Azerbaijani Spanish Sundanese Swahili Swedish Tagalog Tajik Tamil Tatar Telugu Turkish Ukrainian Urdu Uzbek Vietnamese Volap\u00fck Waray-Waray Welsh West Frisian Western Punjabi Yoruba","title":"Supported Languages"}]}